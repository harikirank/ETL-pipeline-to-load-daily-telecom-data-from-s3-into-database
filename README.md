# Telecom Data Ingestion Pipeline
## An AWS data pipeline to incrementally ingest data into an RDS MySQL DB instance in AWS Cloud.

# Prerequisites
* A good understanding of AWS Services: S3, Glue, VPC, RDS
* Good understanding of Python, and SQL.

# Project Motivation
* The main motive behind the project is to understand building an end-to-end data pipeline (extract, transform, load, and visualize) using AWS services.

# Architecture Diagram
![Architecture Diagram](./ArchitectureDiagram/TelecomDataIngestionArchitectureDiagram.png?raw=true)

# Steps to build the pipeline
1. Create an s3 bucket to store the data from the upstream source.
![Step 1 Image](./Images/1_Created_bucket.png?raw=true)
2. Created the partition to store the data for each day. This will reduce the amount of data scan and saves us costs.
![Step 2 Image](./Images/2_Created_Partitions_to_store_the_data_for_each_day.png?raw=true)
3. CSV data for the corresponding day gets dumped into the bucket from the source.
![Step 3 Image](./Images/3_csv_file.png?raw=true)
4. Crawler gets triggered to create a glue catalog of the s3 data.
![Step 4 Image](./Images/4_glue_crawler_for_s3.png?raw=true)
5. Create an RDS MySQL DB to be used as target.
![Step 5 Image](./Images/5_RDS_DB_As_Target.png?raw=true)
6. RDS Crawler to crawl the MySQL DB for getting the metadata of the database.
![Step 6 Image](./Images/6_RDS_Crawler.png?raw=true)
7. Glue Crawler runs succeeded.
![Step 7 Image](./Images/7_two_db_tables_created.png?raw=true)
8. Two catalog database tables will be created as a result of the crawls.
![Step 8 Image](./Images/8_db_tables_that_were_created_from_s3_and_rds.png?raw=true)
9. Creating a glue job to do transformations on the data and load the data into the target database.
![Step 9 Image](./Images/9_GlueETLVisual.png?raw=true)
10. Glue Script that was generated by glue for performing the above transformations.
![Step 10 Image](./Images/10_Glue_Script.png?raw=true)
11. Enabling job bookmarking for incremental data ingestion so that we donâ€™t load duplicate data into our target once we receive the data for the next day(s).
![Step 11 Image](./Images/11_enabling_job_bookmarks.png?raw=true)
12. Data in the RDS table before the glue job has run.
![Step 12 Image](./Images/12_null_data_in_table_before_glue_job_run.png?raw=true)
13. Glue job run successful.
![Step 13 Image](./Images/13_glue_job_run_succeded.png?raw=true)
14. Rows ingested into to the target table by the glue job run.
![Step 14 Image](./Images/14_rows_ingested_by_glue_job_run.png?raw=true)
15. Data from day 2 of the month landed in the s3 bucket.
![Step 15 Image](./Images/15_data_upload_of_the_2nd_day.png?raw=true)
16. Crawler re-ran to update the metadata of the catalog.
![Step 16 Image](./Images/16_crawler_rerun.png?raw=true)
17. Crawler updated the partitions of the catalog.
![Step 17 Image](./Images/17_partitions_for_second_day.png?raw=true)
18. 2nd run of the glue etl job to ingest the data from day 2.
![Step 18 Image](./Images/18_2nd_run_of_glue_etl_job.png?raw=true)
19. Data in the rds db updated with no duplicates ingested. This is because of the job bookmarking feature that we have enabled in the glue job.
![Step 19 Image](./Images/19_updated_rds_no_duplicates.png?raw=true)

    
# Next Steps:
1. Everything can be automated.
2. You can build an event driven architecture. As soon as data lands on the s3 bucket, you can trigger the glue crawler run, followed by glue job run.
